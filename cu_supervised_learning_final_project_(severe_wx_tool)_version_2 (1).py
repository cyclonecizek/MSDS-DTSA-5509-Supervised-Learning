# -*- coding: utf-8 -*-
"""CU Supervised Learning - Final Project (Severe Wx Tool) version 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qPa6PBXxf92N9JR0Z5cpvgYUPpQbGe2d

# Introduction

This is a supervised machine learning project in which I attempt to predict the occurence of severe weather in the wet/summer season in the vicinity of Cape Canaveral using the 1500Z/11:00am ET daily weather balloon. This balloon is a good predictor, since it samples the environment a few hours before thunderstorms typically develop. From the weather balloon data, we can get many candidate predictors, to include parameters that measure moisture, instability and wind speed/direction. From these features, we will create and test several binary classifition models, where the label is a 1 or a 0 on whether Severe Weather occured within 30 nm of Cape Canaveral Space Force Station/Kennedy Space Center. I decided to subset the data to just include days with offshore wind direction to help balance the dataset and since this is the main forecast challenge. Ultimately, our goal is to get a reliable probabilistic forecast for severe weather based on this balloon data.

# Code Preamble
"""

!pip install -U shap

# -*- coding: utf-8 -*-


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

pd.set_option('display.max_rows', 40)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split
from patsy import dmatrices

from sklearn import tree # for decision tree models

import plotly.express as px  # for data visualization
import plotly.graph_objects as go # for data visualization
import graphviz # for plotting decision tree graphs

from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

print("\n---Libraries loaded")

"""# Data Understanding/Preparation"""



"""Don't run this cell if files not saved in google drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Data preparation

Update below with file path where you saved the file
"""

soundingfile = '/content/drive/My Drive/15Z_KXMR_soundingparameters.csv'
#plfile = '/content/drive/My Drive/precipltng_2008_2022.csv'
severe_reports = '/content/drive/My Drive/NWS_severe_reports_2008_2023.csv'
sounding_severe = '/content/drive/My Drive/sounding_severe_30nm_2005_2022.csv'
severe_days = '/content/drive/My Drive/severedates_30nm_2005_2023.csv'

df1 = pd.read_csv(soundingfile)
df2 = pd.read_csv(severe_days)
#df3 = pd.read_csv(severe_reports)


df2.head(10)
#print(df2.head(10))
#print(df3.head(10))

import pandas as pd
df1['Datetime'] = pd.to_datetime(df1[['Year', 'Month', 'Day']])

df1.head(10)

#only include rows that are in the months May through September, inclusive

df1 = df1[(df1['Month'] >= 5) & (df1['Month'] <= 9)]

df2['Date'] = pd.to_datetime(df2['Date'])

df2.head(10)

#remove all rows before the year 2005. This is where we have reliable severe report data

df1 = df1[df1['Year'] >= 2005]

#Insert the df2[Severe] column to df1 where df1[Datetime] = df2[Date]

import pandas as pd
df1 = pd.merge(df1, df2[['Date', 'Severe']], left_on='Datetime', right_on='Date', how='left')
df1.drop('Date', axis=1, inplace=True)

#find how many rows have an Na or inf in the Severe column

import numpy as np
# Count NaNs in 'Severe' column
nan_count = df1['Severe'].isna().sum()

# Count infinities in 'Severe' column
inf_count = np.isinf(df1['Severe']).sum()

print("Number of NaNs in 'Severe' column:", nan_count)
print("Number of infinities in 'Severe' column:", inf_count)

#download df1 as a csv

#from google.colab import files
#df1.to_csv('df1.csv', index=False)
#files.download('df1.csv')

#In each column header in df, remove the text that is in the parentheses, and the parentheses themselves

df1.columns = [col.split(' (')[0] for col in df1.columns]

df1

columns = ['Platform', 'Year', 'Month', 'Elevation', 'Day', 'Hour', 'Minute', 'Sounding Base', 'Sounding Top', 'Sounding Levels', 'Bulk Richardson Number']
df1.drop(columns, inplace=True, axis=1)

df1.head(10)
print("WET SEASON")

df1.head(150)

df1.drop(['Lat', 'Lon'], inplace=True, axis=1)

#make all the column names in df have no spaces within their string

df1.columns = [column.replace(' ', '_') for column in df1.columns]

df1.drop(['Datetime'], inplace=True, axis=1)
df1.drop(['Cloud_Depth_Ratio', 'Equilibrium_Level'], inplace=True, axis=1)

#Save df1 as a csv to my drive named "15Zsounding_severe_30nm_2005_2023.csv"

#df1.to_csv('/content/drive/My Drive/15Zsounding_severe_30nm_2005_2023.csv', index=False)

df1.head(10)

"""Data Description Table"""

df1.info()
df1.describe()
df1.isnull().sum()


#df_warm.info()
#df_warm.describe()
#df_warm.isnull().sum()

rows, columns = df1.shape
cell_count = rows * columns
number_of_nulls = df1.isnull().sum().sum()
percentage_of_missing = (number_of_nulls / cell_count) * 100
print(f'Percentage of missing values: {percentage_of_missing}%')



"""# Data Preparation

Dealing with Null values
"""

#df1['Cloud_Depth_Ratio'].fillna(0, inplace = True)
#df1['Equilibrium_Level'].fillna(0, inplace = True)
#df['Hail'].fillna(0, inplace=True)

df1.dropna(inplace = True)
df1.info()
df1.describe()
df1.isnull().sum()

df1.columns = [col.split('_(')[0] for col in df1.columns]

df1['PWAT'] = df1['PWAT'].astype(float)
df1['PWAT'] = df1['PWAT']* 0.039370 # convert from mm to in
df1['PWAT'].describe()

# Delete rows where 'PWAT' is < 0 or > 3 (not physically possible)
df1 = df1[(df1['PWAT'] >= 0) & (df1['PWAT'] <= 3)]
df1['PWAT'].describe()

df1.head(10)

df1.describe()

df1 = df1[(df1['WINDEX'] >= 0) & (df1['WINDEX'] <= 100)]
df1['WINDEX'].describe()

"""**Create a df for just offshore flow days using the 1000-700mb averaged wind direction**"""

df_offshore = df1[df1['1000-700mb_Average_U-Wind_Component'] > 0]

df_offshore

df_offshore['WINDEX'].describe()

"""# EDA/Visualization

Heat Map Correlation Matrix

**Just offshore flow**
"""

plt.figure(figsize=(30,20))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(df_offshore.corr(),annot=True,fmt='.2f',cmap=cmap )
plt.show()

"""**All days, onshore and offshore flow**"""

plt.figure(figsize=(30,20))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(df1.corr(),annot=True,fmt='.2f',cmap=cmap )
plt.show()

"""Histograms of Each Variable"""

# Create figure and axes with 13 rows and 3 columns
fig, ax = plt.subplots(nrows=14, ncols=3, figsize=(20,60))

# Get the column names
distribution = df_offshore.columns

rows = 0
cols = 0
for i, column in enumerate(distribution):
    # Use histplot instead of displot for better axes control
    sns.histplot(data=df_offshore[column], ax=ax[rows, cols])
    cols += 1
    if cols == 3:
        cols = 0
        rows += 1

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""Breakdown of Severe vs. No Severe days.dataset"""

df1['Severe'].value_counts()

Tasks = [1890, 557]

my_labels = 'Non-Severe Day','Severe Day'
plt.pie(Tasks, labels=my_labels,autopct='%1.1f%%')
plt.title('Severe vs. Non-Severe Days')
plt.axis('equal')
plt.show()

non_severe, severe = df_offshore['Severe'].value_counts()
print(severe)
print(non_severe)

offshore_severe_pct = severe / (severe + non_severe)
print(offshore_severe_pct)



Tasks = [non_severe, severe]

my_labels = 'Non-Severe Day','Severe Day'
plt.pie(Tasks, labels=my_labels,autopct='%1.1f%%')
plt.title('Offshore Severe vs Non-Severe')
plt.axis('equal')
plt.show()

df1.describe()

df_offshore.describe()

# prompt: Count the number of columns in df_offshore

num_columns = len(df_offshore.columns)
print(f"The number of columns in df_offshore is: {num_columns}")

"""# Start Modeling Prep"""



#Split into Train/Test Data
#columns_drop = ['Year', 'Month', 'Day', 'Hour_(UTC)']
#columns_drop = ['Year', 'Month', 'Day', 'Hour (UTC)', 'CAPE', 'PWAT', 'Equilibrium_Level', 'Cloud_Depth_Ratio', 'EHI', 'LI', 'KI', 'Surface_700_RH']
#df.drop(columns_drop, inplace=True, axis=1)

#Model 1 (all variables, no transformations)
X = df1.loc[:, df1.columns != 'Severe']
y = df1.loc[:, df1.columns == 'Severe']

#X = (X-X.mean()) / X.std()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=42)

# prompt: Standardize variables in the X dataset

from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the testing data using the same scaler
X_test_scaled = scaler.transform(X_test)

# Convert the scaled data back to dataframes (optional but recommended)
#X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
#X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

"""Just offshore flow"""

#Model 1 (all variables, no transformations)
X_offshore = df_offshore.loc[:, df_offshore.columns != 'Severe']
y_offshore = df_offshore.loc[:, df_offshore.columns == 'Severe']


#X = (X-X.mean()) / X.std()
X_train_offshore, X_test_offshore, y_train_offshore, y_test_offshore = train_test_split(X_offshore, y_offshore, test_size=0.3, stratify = y_offshore, random_state=42)

# Fit and transform the training data
X_train_offshore_scaled = scaler.fit_transform(X_train_offshore)

# Transform the testing data using the same scaler
X_test_offshore_scaled = scaler.transform(X_test_offshore)

# Convert the scaled data back to dataframes (optional but recommended)
#X_train_scaled_offshore = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
#X_test_scaled_offshore = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

y_test



"""# Logistic Regression All Predictors"""

# First model: Includes all predictor variables with no transforms on the non-resampled dataset
logreg_model1 = sm.Logit(y_train_offshore,X_train_offshore_scaled).fit()
print(logreg_model1.summary())

from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

#Fit the Model with Sklearn
logreg = LogisticRegression(fit_intercept = True)    # default intercept = true
#logreg = LogisticRegression(fit_intercept = False)

logreg.fit(X_train_offshore_scaled, y_train_offshore.values.ravel())
y_pred_test = logreg.predict(X_test_offshore_scaled)
y_pred_train = logreg.predict(X_train_offshore_scaled)
print("Modeling complete")
# Instead of printing summary, you can print coefficients and intercept
print("Coefficients:", logreg.coef_)  # Print the coefficients
print("Intercept:", logreg.intercept_)  # Print the intercept

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(logreg.score(X_test_offshore_scaled, y_test_offshore))

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_test)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_test))

#An attempt at a regression model summary
print("\n---An attempt at a regression model summary")

import sklearn.metrics as metrics
def regression_results(y_true, y_pred):

    # Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred)
    mse=metrics.mean_squared_error(y_true, y_pred)
    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)

    print('explained_variance: ', round(explained_variance,4))
    print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))


regression_results(y_test_offshore, y_pred_test)

#calculate the brier score for the above logistic regression

from sklearn.metrics import brier_score_loss


# Calculate predicted probabilities if you only have predicted classes
y_pred_proba = logreg.predict_proba(X_test_offshore_scaled)[:, 1]


brier_score = brier_score_loss(y_test_offshore, y_pred_proba)

print(f"Brier Score: {brier_score}")

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y_test_offshore, logreg.predict(X_test_offshore_scaled))
fpr, tpr, thresholds = roc_curve(y_test_offshore, logreg.predict_proba(X_test_offshore_scaled)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

"""# P-Value and Meteorological Insight

['CAPE' '700-500mb_Lapse_Rate' '850-500mb_Lapse_Rate' 'LI' 'KI'
 'Thompson_Index' 'Total_Totals' 'T_1000mb' 'T_850mb' 'T_700mb' 'T_500mb'
 'Convective_Temperature' '1000-700mb_RH' 'PWAT' '700-500mb_Average_RH'
 'Surface-700mb_Average_RH' 'Surface-700mb_Average_Wind_Speed'
 '1000-700mb_Average_U-Wind_Component'
 '1000-700mb_Average_V-Wind_Component' '850mb_Average_U-Wind_Component'
 '850mb_Average_V-Wind_Component' '700mb_Average_U-Wind_Component'
 '700mb_Average_V-Wind_Component' '500mb_Average_U-Wind_Component'
 '500mb_Average_V-Wind_Component' '250mb_Average_U-Wind_Component'
 '250mb_Average_V-Wind_Component' '0-6km_Shear'
 '0-3km_Storm_Relative_Helicity' 'Wet_Bulb_Zero_Level' 'Equilibrium_Level'
 'Lowest_Freezing_Level' '-10C_Level' 'Cloud_Depth_Ratio'
 'Energy_Helicity_Index' 'Precip']
"""

formula = 'Severe ~ Thompson_Index + WINDEX + Q("1000-700mb_Average_U-Wind_Component") '
y1, X1 = dmatrices(formula, data = df_offshore , return_type ='dataframe')
#X1 = (X1-X1.mean())/X1.std()
#X1['Intercept'] = 1

X1_train , X1_test , y1_train , y1_test = train_test_split ( X1, y1, test_size =0.2 , stratify = y1, random_state =42)
X1_train.head(10)

#from imblearn.over_sampling import SMOTE
#smote = SMOTE(random_state=42)
#X1_res, y1_res = smote.fit_resample(X1_train, y1_train)

model1 = sm.Logit(y1_train,X1_train).fit()
print(model1.summary())

from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

#Fit the Model with Sklearn
#logreg = LogisticRegression(fit_intercept = True)    # default intercept = true
logreg = LogisticRegression(fit_intercept = False)

logreg.fit(X1_train, y1_train.values.ravel())
y1_pred_test = logreg.predict(X1_test)
y1_pred_train = logreg.predict(X1_train)
print("Modeling complete")
y1_pred_test
logreg_predict = logreg.predict_proba(X1_test)
print(type(logreg_predict))

df_logreg_predict = pd.DataFrame(logreg_predict)
df_logreg_predict
#df_logreg_predict['lightning'] = y1_test['Lightning']
df_logreg_predict
y1_test_reset = y1_test.reset_index()
df_logreg_predict = pd.concat([df_logreg_predict, y1_test_reset['Severe']], axis=1)
df_logreg_predict

#y1_test['Lightning']

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(logreg.score(X1_test, y1_test))

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y1_test, y1_pred_test)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y1_test, y1_pred_test))

#An attempt at a regression model summary
print("\n---An attempt at a regression model summary")

import sklearn.metrics as metrics
def regression_results(y_true, y_pred):

    # Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred)
    mse=metrics.mean_squared_error(y_true, y_pred)
    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)

    print('explained_variance: ', round(explained_variance,4))
    print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))


regression_results(y1_test, y1_pred_test)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y1_test, logreg.predict(X1_test))
fpr, tpr, thresholds = roc_curve(y1_test, logreg.predict_proba(X1_test)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

"""# Logistic Regression with Recursive Feature Elimination"""

from sklearn.feature_selection import RFE
from sklearn import linear_model
model_a = linear_model.LogisticRegression()



rfe = RFE(model_a, n_features_to_select=8, step = 1)
fit = rfe.fit(X_train, y_train.values.ravel())

f = fit.get_support(1) #the most important features
# final_features = data[data.columns[f]] # final features: this gives wrong results
final_features = X[X.columns[f]] # final features

print("Num Features: %d" % fit.n_features_)
print("Selected Features: %s" % final_features.columns)
print("Score: %2.3f" % fit.score(X,y.values.ravel()))

print("----------")

#Selected 'Ed', 'LF', 'MF', 'Wealth', 'Time'

# summarize all features
for i in range(X.shape[1]):
	print('Column: %d, %s, Selected %s, Rank: %.3f' % (i, X.columns.values[i],rfe.support_[i], rfe.ranking_[i]))

model = linear_model.LogisticRegression()


print("\nSearch the best k features for k = 1 to 27\n")

for k in range(1, 10):
  rfe = RFE(model, n_features_to_select = k, step=1)
  fit = rfe.fit(X, y)

  f = fit.get_support(1) #the most important features

  # final_features = data[data.columns[f]] # final features: this gives wrong results
  final_features = X[X.columns[f]] # final features

  print("Num Features: %d" % fit.n_features_)
  print("Selected Features: %s" % final_features.columns)
  print("Score: %2.2f" % fit.score(X,y))
  print("----------")

formula2 = 'Severe ~ Thompson_Index + Convective_Temperature + T_500mb + Q("1000-850mb_Average_U-Wind_Component") + Q("850-500mb_Average_V-Wind_Component")'
y2, X2 = dmatrices(formula2, data = df_offshore , return_type ='dataframe')
#X1 = (X1-X1.mean())/X1.std()
#X1['Intercept'] = 1

X2_train , X2_test , y2_train , y2_test = train_test_split ( X2, y2, test_size =0.3 , stratify = y1, random_state =42)
X2_train.head(10)

model2 = sm.Logit(y2_train,X2_train).fit()
print(model2.summary())

#Fit the Model with Sklearn
logreg = LogisticRegression(fit_intercept = True)    # default intercept = true
#logreg = LogisticRegression(fit_intercept = False)

logreg.fit(X2_train,y2_train.values.ravel())
y2_pred_test = logreg.predict(X2_test)
y2_pred_train = logreg.predict(X2_train)
print("Modeling complete")

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(logreg.score(X2_test, y2_test))

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y2_test, y2_pred_test)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y2_test, y2_pred_test))

#An attempt at a regression model summary
print("\n---An attempt at a regression model summary")

import sklearn.metrics as metrics
def regression_results(y_true, y_pred):

    # Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred)
    mse=metrics.mean_squared_error(y_true, y_pred)
    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)

    print('explained_variance: ', round(explained_variance,4))
    print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))


regression_results(y2_test, y2_pred_test)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

logit_roc_auc = roc_auc_score(y2_test, logreg.predict(X2_test))
fpr, tpr, thresholds = roc_curve(y2_test, logreg.predict_proba(X2_test)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()



"""# Decision Tree / CART Approach

This code is modified from https://towardsdatascience.com/cart-classification-and-regression-trees-for-clean-but-powerful-models-cc89e60b7a85
"""

from sklearn.tree import DecisionTreeClassifier

def fitting(X, y, criterion, splitter, mdepth, clweight, minleaf):

    # Create training and testing samples
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=42)

    #Apply SMOTE

    #smote = SMOTE(random_state=42)
    #X_train, y_train = smote.fit_resample(X_train, y_train)

    # Fit the model
    model = tree.DecisionTreeClassifier(criterion=criterion,
                                        splitter=splitter,
                                        max_depth=mdepth,
                                        class_weight=clweight,
                                        min_samples_leaf=minleaf,
                                        random_state=0,
                                  )
    clf = model.fit(X_train, y_train)



    # Predict class labels on training data
    pred_labels_tr = model.predict(X_train)
    #pred_labels_tr = (model.predict_proba(X_train)[:,1] >= 0.2).astype(bool)
    # Predict class labels on a test data
    pred_labels_te = model.predict(X_test)
    #pred_labels_te = (model.predict_proba(X_test)[:,1] >= 0.4).astype(bool)

    # Tree summary and model evaluation metrics
    print('*************** Tree Summary ***************')
    print('Classes: ', clf.classes_)
    print('Tree Depth: ', clf.tree_.max_depth)
    print('No. of leaves: ', clf.tree_.n_leaves)
    #print('No. of features: ', clf.n_features_)
    print('--------------------------------------------------------')
    print("")

    print('*************** Evaluation on Test Data ***************')
    score_te = model.score(X_test, y_test)
    print('Accuracy Score: ', score_te)
    # Look at classification report to evaluate the model
    print(classification_report(y_test, pred_labels_te))
    print('--------------------------------------------------------')
    print("")

    print('*************** Evaluation on Training Data ***************')
    score_tr = model.score(X_train, y_train)
    print('Accuracy Score: ', score_tr)
    # Look at classification report to evaluate the model
    print(classification_report(y_train, pred_labels_tr))
    print('--------------------------------------------------------')

    print('*************** Feature importance ***************')
    feature_importance = clf.feature_importances_
    print('Feature Importance: ', feature_importance)

    #Plot feature importance
    feature_imp = pd.Series(clf.feature_importances_, index = clf.feature_names_in_)
    plt.figure(figsize=(20,15))
    sns.barplot(x=feature_imp, y=feature_imp.index)
    plt.xlabel('Feature Importance Score')
    plt.ylabel('Features')
    plt.title("Visualizing Important Features", pad=15, size=14)

    #Make the Roc Curve for test data
    rfc_roc_auc = roc_auc_score(y_test, clf.predict(X_test))
    fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])

    plt.figure()
    plt.plot(fpr, tpr, label='CART (area = %0.2f)' % rfc_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC', transparent=True)
    plt.show()

    # Use graphviz to plot the tree
    dot_data = tree.export_graphviz(clf, out_file=None,
                                feature_names=X.columns,
                                class_names=[str(list(clf.classes_)[0]), str(list(clf.classes_)[1])],
                                filled=True,
                                rounded=True,
                                #rotate=True,
                               )
    graph = graphviz.Source(dot_data)

    # Return relevant data for chart plotting
    return X_train, X_test, y_train, y_test, clf, graph

# Fit the model and display results
X_train, X_test, y_train, y_test, clf, graph = fitting(X_offshore, y_offshore, 'entropy', 'best',
                                                       mdepth=10,
                                                       clweight=None,
                                                       minleaf=10)

# Plot the tree graph
graph


# Save tree graph to a PDF
#graph.render('Decision_Tree_all_vars_gini')

X_cart = X_train_offshore[['Thompson_Index', 'WINDEX', '850-500mb_Average_U-Wind_Component', 'Total_Totals','Surface-700mb_Average_Wind_Speed']].copy()


X_train, X_test, y_train, y_test, clf, graph = fitting(X_cart, y_train_offshore, 'entropy', 'best',
                                                       mdepth=15,
                                                       clweight=None,
                                                       minleaf=100)

# Plot the tree graph
graph

"""# Random Forest Classifier"""

def hss(a, b, c, d):
  return (2*(a*d)-(b*c))/(((a+c)*(c+d)) + ((a + b)*(b+d)))

# prompt: Create a 1d array the same length as y_test, but with the value offshore_severe_pct as every value

import numpy as np

# Create a 1d array the same length as y_test
offshore_severe_pct_array = np.full(len(y_test), offshore_severe_pct)

total_severe_pct_array = np.full(len(y_test), .223)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import brier_score_loss

RFC_model = RandomForestClassifier(n_estimators=500,  criterion='entropy', max_features='sqrt',
                                 max_depth=6, bootstrap=True, n_jobs=1,
                                 random_state=42)

RFC_model.fit(X_train, y_train.values.ravel())
y_pred = RFC_model.predict(X_test)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model.score(X_test, y_test))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model.predict_proba(X_test)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
#probs = RFC_model.predict_proba(X_test)
# keep the predictions for class 1 only
#probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test), offshore_severe_pct)
brier_climo = brier_score_loss(y_test, total_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

hss1 = hss(15, 93, 7, 473)
print(hss1)

hss2 = hss(6, 25, 9, 352)
print(hss2)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

rfc_roc_auc = roc_auc_score(y_test, RFC_model.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, RFC_model.predict_proba(X_test)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='RFC (area = %0.2f)' % rfc_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

#feature_imp = pd.Series(model.feature_importances_,
                        #index=['sepal length (cm)', 'sepal width (cm)',
#'petal length (cm)', 'petal width (cm)']).sort_values(ascending=False)

feature_imp = pd.Series(RFC_model.feature_importances_, index = RFC_model.feature_names_in_)


plt.figure(figsize=(20,15))
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features", pad=15, size=14)

print(RFC_model.feature_names_in_)

RFC_model_weights_full = RandomForestClassifier(n_estimators=500, class_weight= 'balanced', criterion='entropy', max_features='sqrt',
                                 max_depth = 8, bootstrap=True, n_jobs=1,
                                 random_state=9)

RFC_model_weights_full.fit(X_train, y_train.values.ravel())
y_pred = RFC_model_weights_full.predict(X_test)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model_weights_full.score(X_test, y_test))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model_weights_full.predict_proba(X_test)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = RFC_model_weights_full.predict_proba(X_test)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

#offshore_severe_pct_array = np.full(len(y_test), severe_pct)
#brier_climo = brier_score_loss(y_test, severe_pct_array)
#BSS = 1 - (brier_raw / brier_climo)
#print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

X_train_RFC = X_train[['Thompson_Index', '850mb_Average_U-Wind_Component','700-500mb_Average_RH', 'WINDEX']].copy()
X_test_RFC = X_test[['Thompson_Index', '850mb_Average_U-Wind_Component', '700-500mb_Average_RH', 'WINDEX']].copy()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import brier_score_loss

RFC_model = RandomForestClassifier(n_estimators=500,  criterion='entropy', max_features='sqrt',
                                 max_depth=6, bootstrap=True, n_jobs=1,
                                 random_state=42)

RFC_model.fit(X_train_RFC, y_train.values.ravel())
y_pred = RFC_model.predict(X_test_RFC)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model.score(X_test_RFC, y_test))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model.predict_proba(X_test_RFC)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
#probs = RFC_model.predict_proba(X_test)
# keep the predictions for class 1 only
#probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test), offshore_severe_pct)
brier_climo = brier_score_loss(y_test, total_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

"""#Random Forest Just Offshore Flow"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import brier_score_loss

RFC_model_offshore = RandomForestClassifier(n_estimators=500,  criterion='entropy', max_features='sqrt',
                                 max_depth = 8, bootstrap=True, n_jobs=1,
                                 random_state=9)

RFC_model_offshore.fit(X_train_offshore, y_train_offshore.values.ravel())
y_pred_offshore = RFC_model_offshore.predict(X_test_offshore)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model_offshore.score(X_test_offshore, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model_offshore.predict_proba(X_test_offshore)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = RFC_model_offshore.predict_proba(X_test)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

rfc_roc_auc = roc_auc_score(y_test_offshore, RFC_model_offshore.predict(X_test_offshore))
fpr, tpr, thresholds = roc_curve(y_test_offshore, RFC_model_offshore.predict_proba(X_test_offshore)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='RFC (area = %0.2f)' % rfc_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

feature_imp = pd.Series(RFC_model_offshore.feature_importances_, index = RFC_model_offshore.feature_names_in_)


plt.figure(figsize=(20,15))
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features", pad=15, size=14)

RFC_model_offshore_weights_full = RandomForestClassifier(n_estimators=500, class_weight= 'balanced', criterion='entropy', max_features='sqrt',
                                 max_depth = 8, bootstrap=True, n_jobs=1,
                                 random_state=9)

RFC_model_offshore_weights_full.fit(X_train_offshore, y_train_offshore.values.ravel())
y_pred_offshore = RFC_model_offshore_weights_full.predict(X_test_offshore)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model_offshore_weights_full.score(X_test_offshore, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model_offshore_weights_full.predict_proba(X_test_offshore)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = RFC_model_offshore_weights_full.predict_proba(X_test_offshore)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

rfc_roc_auc = roc_auc_score(y_test_offshore, RFC_model_offshore_weights_full.predict(X_test_offshore))
fpr, tpr, thresholds = roc_curve(y_test_offshore, RFC_model_offshore_weights_full.predict_proba(X_test_offshore)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='RFC (area = %0.2f)' % rfc_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

"""#Feature Importance Just Offshore Flow Random Forest"""

import numpy as np
import pandas as pd
import shap

from sklearn.inspection import permutation_importance
from matplotlib import pyplot as plt
import seaborn as sns # for correlation heatmap

from xgboost import XGBRegressor

perm_importance = permutation_importance(RFC_model_offshore, X_test_offshore, y_test_offshore)

sorted_idx = perm_importance.importances_mean.argsort()

plt.figure(figsize=(20, 15))
plt.barh(RFC_model_offshore.feature_names_in_[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
plt.show()

sorted_idx = RFC_model_offshore.feature_importances_.argsort()
plt.figure(figsize=(20, 15))
plt.barh(RFC_model_offshore.feature_names_in_[sorted_idx], RFC_model_offshore.feature_importances_[sorted_idx])
plt.xlabel("RFC Offshore Feature Importance")

explainer = shap.TreeExplainer(RFC_model_offshore)
shap_values = explainer.shap_values(X_test_offshore)

shap.summary_plot(shap_values, X_test_offshore, plot_type="bar")

"""#Random Forest Offshore Flow With Limited Features"""

#X_train_limited = X_train_offshore[['Thompson_Index', '700-500mb_Average_RH',  'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component', '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Wet_Bulb_Zero_Level']].copy()
#X_test_limited = X_test_offshore[['Thompson_Index', '700-500mb_Average_RH',  'PWAT', 'WINDEX', '1000-850mb_Average_U-Wind_Component', '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Wet_Bulb_Zero_Level']].copy()

X_train_limited = X_train_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()
X_test_limited = X_test_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()

#X_train_limited = X_train_offshore[['Thompson_Index', '700-500mb_Average_RH', 'PWAT', 'WINDEX',  '1000-700mb_Average_U-Wind_Component']].copy()
#X_test_limited = X_test_offshore[['Thompson_Index', '700-500mb_Average_RH', 'PWAT', 'WINDEX', '1000-700mb_Average_U-Wind_Component']].copy()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import brier_score_loss

RFC_model_offshore = RandomForestClassifier(n_estimators=500,  criterion='entropy', max_features='sqrt',
                                 max_depth = 8, bootstrap=True, n_jobs=1,
                                 random_state=9)

RFC_model_offshore.fit(X_train_limited, y_train_offshore.values.ravel())
y_pred_offshore = RFC_model_offshore.predict(X_test_limited)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model_offshore.score(X_test_limited, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model_offshore.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = RFC_model_offshore.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

RFC_model_offshore_weights = RandomForestClassifier(n_estimators=500, class_weight= 'balanced', criterion='entropy', max_features='sqrt',
                                 max_depth = 8, bootstrap=True, n_jobs=1,
                                 random_state=9)

RFC_model_offshore_weights.fit(X_train_limited, y_train_offshore.values.ravel())
y_pred_offshore = RFC_model_offshore_weights.predict(X_test_limited)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(RFC_model_offshore_weights.score(X_test_limited, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = RFC_model_offshore_weights.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = RFC_model_offshore_weights.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

#Make the Roc Curve for test data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

rfc_roc_auc = roc_auc_score(y_test_offshore, RFC_model_offshore_weights.predict(X_test_limited))
fpr, tpr, thresholds = roc_curve(y_test_offshore, RFC_model_offshore_weights.predict_proba(X_test_limited)[:,1])

plt.figure()
plt.plot(fpr, tpr, label='RFC (area = %0.2f)' % rfc_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC', transparent=True)
plt.show()

feature_imp = pd.Series(RFC_model_offshore.feature_importances_, index = RFC_model_offshore.feature_names_in_)


plt.figure(figsize=(20,15))
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features", pad=15, size=14)

# prompt: Do a hyperparameter sweep on the Random Forest model RFC_model_offshore to maximize the f1-score on the test dataset

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the grid search object
grid_search = GridSearchCV(RFC_model_offshore, param_grid, scoring='f1', cv=5)

# Fit the grid search object to the training data
grid_search.fit(X_train_offshore, y_train_offshore)

# Get the best model
best_model = grid_search.best_estimator_

# Predict on the test data
y_pred = best_model.predict(X_test_offshore)

# Calculate the f1-score
f1 = f1_score(y_test_offshore, y_pred_offshore)

# Print the f1-score
print(f"Best f1-score: {f1}")

# Print the best hyperparameters
print(f"Best hyperparameters: {grid_search.best_params_}")

RFC_model_offshore.fit(X_train_limited, y_train_offshore.values.ravel())
y_pred_offshore = RFC_model_offshore.predict(X_test_limited)

"""Calibrating the Random Forest Classifier"""

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from sklearn.calibration import CalibrationDisplay

# Generate predictions
y_prob = RFC_model_offshore.predict_proba(X_test_limited)[:, 1]  # Probability for the positive class

# Create figure and grid
fig = plt.figure(figsize=(10, 10))
gs = GridSpec(4, 2)
color = plt.cm.Dark2(0)  # Single color for the RFC plot

# Calibration curve plot
ax_calibration_curve = fig.add_subplot(gs[:2, :2])
CalibrationDisplay.from_predictions(
    y_test_offshore,
    y_prob,
    n_bins=10,
    name="Random Forest Classifier",
    ax=ax_calibration_curve,
    color=color,
)
ax_calibration_curve.grid()
ax_calibration_curve.set_title("Calibration Plot (Random Forest Classifier)")

# Histogram plot
ax_hist = fig.add_subplot(gs[2:, :])
ax_hist.hist(
    y_prob,
    range=(0, 1),
    bins=10,
    color=color,
    edgecolor='black'
)
ax_hist.set(
    title="Predicted Probability Distribution (RFC)",
    xlabel="Mean Predicted Probability",
    ylabel="Count"
)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from sklearn.calibration import CalibrationDisplay

# Generate predictions
y_prob = RFC_model_offshore_weights.predict_proba(X_test_limited)[:, 1]  # Probability for the positive class

# Create figure and grid
fig = plt.figure(figsize=(10, 10))
gs = GridSpec(4, 2)
color = plt.cm.Dark2(0)  # Single color for the RFC plot

# Calibration curve plot
ax_calibration_curve = fig.add_subplot(gs[:2, :2])
CalibrationDisplay.from_predictions(
    y_test_offshore,
    y_prob,
    n_bins=10,
    name="Random Forest Classifier",
    ax=ax_calibration_curve,
    color=color,
)
ax_calibration_curve.grid()
ax_calibration_curve.set_title("Calibration Plot (Random Forest Classifier)")

# Histogram plot
ax_hist = fig.add_subplot(gs[2:, :])
ax_hist.hist(
    y_prob,
    range=(0, 1),
    bins=10,
    color=color,
    edgecolor='black'
)
ax_hist.set(
    title="Predicted Probability Distribution (RFC)",
    xlabel="Mean Predicted Probability",
    ylabel="Count"
)

plt.tight_layout()
plt.show()

from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import brier_score_loss
#from sklearn.naive_bayes import GaussianNB

# With no calibration
prob_no_calib = RFC_model_offshore.predict_proba(X_test_limited)[:, 1]

# With isotonic calibration
RFC_isotonic = CalibratedClassifierCV(RFC_model_offshore, cv=5, method="isotonic")
RFC_isotonic.fit(X_train_limited, y_train_offshore)
prob_pos_isotonic = RFC_isotonic.predict_proba(X_test_limited)[:, 1]

# With sigmoid calibration
RFC_sigmoid = CalibratedClassifierCV(RFC_model_offshore, cv=5, method="sigmoid")
RFC_sigmoid.fit(X_train_limited, y_train_offshore)
prob_pos_sigmoid = RFC_sigmoid.predict_proba(X_test_limited)[:, 1]

print("Brier score losses: (the smaller the better)")

clf_score = brier_score_loss(y_test_offshore, prob_no_calib)
print("No calibration: %1.3f" % clf_score)

RFC_isotonic_score = brier_score_loss(y_test_offshore, prob_pos_isotonic)
print("With isotonic calibration: %1.3f" % RFC_isotonic_score)

RFC_sigmoid_score = brier_score_loss(y_test_offshore, prob_pos_sigmoid)
print("With sigmoid calibration: %1.3f" % RFC_sigmoid_score)

from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import brier_score_loss
#from sklearn.naive_bayes import GaussianNB

# With no calibration
prob_no_calib = RFC_model_offshore_weights.predict_proba(X_test_limited)[:, 1]

# With isotonic calibration
RFC_isotonic = CalibratedClassifierCV(RFC_model_offshore_weights, cv=5, method="isotonic")
RFC_isotonic.fit(X_train_limited, y_train_offshore)
prob_pos_isotonic = RFC_isotonic.predict_proba(X_test_limited)[:, 1]

# With sigmoid calibration
RFC_sigmoid = CalibratedClassifierCV(RFC_model_offshore_weights, cv=5, method="sigmoid")
RFC_sigmoid.fit(X_train_limited, y_train_offshore)
prob_pos_sigmoid = RFC_sigmoid.predict_proba(X_test_limited)[:, 1]

print("Brier score losses: (the smaller the better)")

clf_score = brier_score_loss(y_test_offshore, prob_no_calib)
print("No calibration: %1.3f" % clf_score)

RFC_isotonic_score = brier_score_loss(y_test_offshore, prob_pos_isotonic)
print("With isotonic calibration: %1.3f" % RFC_isotonic_score)

RFC_sigmoid_score = brier_score_loss(y_test_offshore, prob_pos_sigmoid)
print("With sigmoid calibration: %1.3f" % RFC_sigmoid_score)

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from sklearn.calibration import CalibrationDisplay

# Generate predictions
y_prob = RFC_isotonic.predict_proba(X_test_limited)[:, 1]  # Probability for the positive class

# Create figure and grid
fig = plt.figure(figsize=(10, 10))
gs = GridSpec(4, 2)
color = plt.cm.Dark2(0)  # Single color for the RFC plot

# Calibration curve plot
ax_calibration_curve = fig.add_subplot(gs[:2, :2])
CalibrationDisplay.from_predictions(
    y_test_offshore,
    y_prob,
    n_bins=10,
    name="Random Forest Classifier",
    ax=ax_calibration_curve,
    color=color,
)
ax_calibration_curve.grid()
ax_calibration_curve.set_title("Calibration Plot (Random Forest Classifier)")

# Histogram plot
ax_hist = fig.add_subplot(gs[2:, :])
ax_hist.hist(
    y_prob,
    range=(0, 1),
    bins=10,
    color=color,
    edgecolor='black'
)
ax_hist.set(
    title="Predicted Probability Distribution (RFC)",
    xlabel="Mean Predicted Probability",
    ylabel="Count"
)

plt.tight_layout()
plt.show()

"""# XG Classifier"""

from sklearn.ensemble import GradientBoostingClassifier

gradient_booster = GradientBoostingClassifier(learning_rate=0.1, random_state=39)
gradient_booster.get_params()

#gradient_booster.fit(X_train,y_train)
gradient_booster.fit(X_train_offshore,y_train_offshore)
#RFC_model.fit(X_train_RFC, y_train)
#y_pred = RFC_model.predict(X_test_RFC)
print(classification_report(y_test,gradient_booster.predict(X_test)))

gradient_booster.fit(X_train_offshore,y_train_offshore)
y_pred_offshore = gradient_booster.predict(X_test_offshore)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(gradient_booster.score(X_test_offshore, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = gradient_booster.predict_proba(X_test_offshore)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")

# calculate brier skillscore
#offshore_severe_pct_array = np.full(len(y_test), offshore_severe_pct)
#brier_climo = brier_score_loss(y_test, offshore_severe_pct_array)
#BSS = 1 - (brier_raw / brier_climo)
#print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))
#RFC_model.predict_proba(X_test_RFC)

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

sorted_idx = gradient_booster.feature_importances_.argsort()

plt.barh(gradient_booster.feature_names_in_[sorted_idx], gradient_booster.feature_importances_[sorted_idx])
plt.xlabel("Xgboost Feature Importance")
#index = RFC_model_offshore.feature_names_in_

import numpy as np
import pandas as pd
import shap

from sklearn.inspection import permutation_importance
from matplotlib import pyplot as plt
import seaborn as sns # for correlation heatmap

from xgboost import XGBRegressor

perm_importance = permutation_importance(gradient_booster, X_test_offshore, y_test_offshore)

perm_importance.importances_mean[sorted_idx]

sorted_idx = perm_importance.importances_mean.argsort()

plt.figure(figsize=(20, 15))
plt.barh(gradient_booster.feature_names_in_[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
plt.show()

sorted_idx

# prompt: Use the same code as above, but make the image output bigger

import matplotlib.pyplot as plt

# Make the figure larger
plt.figure(figsize=(20, 15))

# Use the same code as above to plot the feature importances
sorted_idx = gradient_booster.feature_importances_.argsort()
plt.barh(gradient_booster.feature_names_in_[sorted_idx], gradient_booster.feature_importances_[sorted_idx])
plt.xlabel("Xgboost Feature Importance")

# Show the plot
plt.show()

from sklearn.metrics import accuracy_score, f1_score, brier_score_loss
from sklearn.feature_selection import SelectFromModel
from numpy import sort

import warnings
warnings.filterwarnings("ignore")

# make predictions for test data and evaluate
predictions = gradient_booster.predict(X_test_offshore)
predictions_prob = gradient_booster.predict_proba(X_test_offshore)[:, 1]
accuracy = accuracy_score(y_test_offshore, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# Fit model using each importance as a threshold
thresholds = sort(gradient_booster.feature_importances_)
for thresh in thresholds:
 # select features using threshold
 selection = SelectFromModel(gradient_booster, threshold=thresh, prefit=True)
 select_X_train = selection.transform(X_train_offshore)
 # train model
 selection_model = GradientBoostingClassifier(learning_rate=0.1, random_state=39)
 selection_model.fit(select_X_train, y_train_offshore)
 # eval model
 select_X_test = selection.transform(X_test_offshore)
 predictions = selection_model.predict(select_X_test)
 predictions_prob = selection_model.predict_proba(select_X_test)[:, 1]
 accuracy = accuracy_score(y_test_offshore, predictions)
 f1score = f1_score(y_test_offshore, predictions)
 brier = brier_score_loss(y_test_offshore, predictions_prob)

 print("Thresh=%.3f, n=%d, Accuracy: %.2f%%, f1 Score: %.2f%%, brier: %.3f  " % (thresh, select_X_train.shape[1], accuracy*100.0, f1score*100.0, brier ))

# prompt: Find the optimal combination of the above features to maximize the f1 score and minimize the brier score. Test all number of features from 15 of them to just 1. Also output what those features names are.

from sklearn.feature_selection import SelectKBest
from sklearn.metrics import f1_score
from sklearn.metrics import brier_score_loss

# Initialize variables
max_f1 = 0
min_brier = 100
best_features = []

# Loop through all possible number of features
for num_features in range(1, 30):
    # Select the top num_features features
    selector = SelectKBest(k=num_features)
    X_train_selected = selector.fit_transform(X_train_offshore, y_train_offshore)
    X_test_selected = selector.transform(X_test_offshore)

    # Train the model
    gradient_booster.fit(X_train_selected, y_train_offshore)

    # Make predictions
    predictions = gradient_booster.predict(X_test_selected)
    predictions_prob = gradient_booster.predict_proba(X_test_selected)[:, 1]

    # Calculate the f1 score and brier score
    f1 = f1_score(y_test_offshore, predictions)
    brier = brier_score_loss(y_test_offshore, predictions_prob)

    # Check if the f1 score is the highest or the brier score is the lowest
    if f1 > max_f1:
        max_f1 = f1
        best_features = selector.get_support()
    if brier < min_brier:
        min_brier = brier
        best_features = selector.get_support()

# Print the results
print("Maximum f1 score:", max_f1)
print("Minimum brier score:", min_brier)
print("Best features:", X_train_offshore.columns[best_features])

explainer = shap.TreeExplainer(gradient_booster)
shap_values = explainer.shap_values(X_test_offshore)

shap.summary_plot(shap_values, X_test_offshore, plot_type="bar")

shap.summary_plot(shap_values, X_test_offshore)

shap_values

from sklearn.metrics import accuracy_score, f1_score, brier_score_loss
from sklearn.feature_selection import SelectFromModel
from numpy import sort

import warnings
warnings.filterwarnings("ignore")

# make predictions for test data and evaluate
predictions = gradient_booster.predict(X_test_offshore)
predictions_prob = gradient_booster.predict_proba(X_test_offshore)[:, 1]
accuracy = accuracy_score(y_test_offshore, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# Fit model using each importance as a threshold
thresholds = sort(gradient_booster.feature_importances_)
for thresh in thresholds:
 # select features using threshold
 selection = SelectFromModel(gradient_booster, threshold=thresh, prefit=True)
 select_X_train = selection.transform(X_train_offshore)
 # train model
 selection_model = GradientBoostingClassifier(learning_rate=0.1, random_state=39)
 selection_model.fit(select_X_train, y_train_offshore)
 # eval model
 select_X_test = selection.transform(X_test_offshore)
 predictions = selection_model.predict(select_X_test)
 predictions_prob = selection_model.predict_proba(select_X_test)[:, 1]
 accuracy = accuracy_score(y_test_offshore, predictions)
 f1score = f1_score(y_test_offshore, predictions)
 brier = brier_score_loss(y_test_offshore, predictions_prob)

 print("Thresh=%.3f, n=%d, Accuracy: %.2f%%, f1 Score: %.2f%%, brier: %.3f  " % (thresh, select_X_train.shape[1], accuracy*100.0, f1score*100.0, brier ))

"""#XGBoost Offshore Limited"""

X_train_limited = X_train_offshore[['CAPE', 'Thompson_Index', '700-500mb_Average_RH', '850-500mb_Lapse_Rate',  'Wet_Bulb_Zero_Level', '-10C_Level',  '1000-700mb_Average_U-Wind_Component']].copy()
X_test_limited = X_test_offshore[['CAPE', 'Thompson_Index', '700-500mb_Average_RH', '850-500mb_Lapse_Rate',  'Wet_Bulb_Zero_Level', '-10C_Level', '1000-700mb_Average_U-Wind_Component']].copy()

X_train_limited = X_train_offshore[['1000-700mb_Average_U-Wind_Component', '850mb_Average_V-Wind_Component', '500mb_Average_U-Wind_Component','CAPE', 'Thompson_Index', '700-500mb_Average_RH',  '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level']].copy()
X_test_limited = X_test_offshore[['1000-700mb_Average_U-Wind_Component', '850mb_Average_V-Wind_Component', '500mb_Average_U-Wind_Component','CAPE', 'Thompson_Index', '700-500mb_Average_RH',  '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level']].copy()



X_train_limited = X_train_offshore[['1000-700mb_Average_U-Wind_Component', 'Thompson_Index', '700-500mb_Average_RH',  '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level']].copy()
X_test_limited = X_test_offshore[['1000-700mb_Average_U-Wind_Component', 'Thompson_Index', '700-500mb_Average_RH',  '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level']].copy()

X_train_limited = X_train_offshore[['WINDEX', 'PWAT', 'Thompson_Index', '700-500mb_Average_RH', '850-500mb_Lapse_Rate',  '850-500mb_Average_U-Wind_Component', '1000-700mb_Average_V-Wind_Component' ]].copy()
X_test_limited = X_test_offshore[['WINDEX', 'PWAT', 'Thompson_Index', '700-500mb_Average_RH', '850-500mb_Lapse_Rate','850-500mb_Average_U-Wind_Component', '1000-700mb_Average_V-Wind_Component']].copy()

X_train_limited = X_train_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()
X_test_limited = X_test_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()

X_train_limited = X_train_offshore[['Thompson_Index', 'WINDEX', 'Surface-700_Average_RH','1000-700mb_Average_U-Wind_Component',  'Wet_Bulb_Zero_Level']].copy()
X_test_limited = X_test_offshore[['Thompson_Index', 'PWAT', 'WINDEX', '1000-700mb_Average_U-Wind_Component',  'Wet_Bulb_Zero_Level']].copy()

"""Last updated"""

X_train_limited = X_train_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()
X_test_limited = X_test_offshore[['Thompson_Index', 'Surface-700mb_Average_RH', '700-500mb_Average_RH',   'PWAT', 'WINDEX',  '1000-850mb_Average_U-Wind_Component',  '850-500mb_Average_U-Wind_Component', '1000-850mb_Average_V-Wind_Component', 'Energy_Helicity_Index', '850-500mb_Lapse_Rate', 'Wet_Bulb_Zero_Level', '0-6km_Shear']].copy()

from sklearn.metrics import brier_score_loss

'''RFC_model_offshore = RandomForestClassifier(n_estimators=500,  criterion='entropy', max_features='auto',
                                 bootstrap=True, n_jobs=1,
                                 random_state=9)'''


gradient_booster = GradientBoostingClassifier(learning_rate=0.1, random_state=39)
gradient_booster.fit(X_train_limited, y_train_offshore)
y_pred_offshore = gradient_booster.predict(X_test_limited)

#RFC_model_offshore.fit(X_train_limited, y_train_offshore.values.ravel())
#y_pred_offshore = RFC_model_offshore.predict(X_test_limited)

#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(gradient_booster.score(X_test_limited, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = gradient_booster.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = gradient_booster.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)

sorted_idx = gradient_booster.feature_importances_.argsort()
plt.barh(gradient_booster.feature_names_in_[sorted_idx], gradient_booster.feature_importances_[sorted_idx])
plt.xlabel("Xgboost Feature Importance")
#index = RFC_model_offshore.feature_names_in_

# prompt: On the cell's above gradient booster model gradient_booster, perform a hyperparamter sweep that minimizes the Brier score

from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create the grid search object
grid_search = GridSearchCV(gradient_booster, param_grid, scoring='neg_brier_score', cv=5)

# Fit the grid search object to the training data
grid_search.fit(X_train_limited, y_train_offshore)

# Get the best model
best_model = grid_search.best_estimator_

# Predict on the test data
y_pred = best_model.predict(X_test_limited)

# Evaluate the model
brier_score = brier_score_loss(y_test_limited, y_pred)
print("Brier score:", brier_score)

# Print the best Brier score
print("Best Brier score:", grid_search.best_score_)

# Print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from sklearn.calibration import CalibrationDisplay

# Generate predictions
y_prob = gradient_booster.predict_proba(X_test_limited)[:, 1]  # Probability for the positive class

# Create figure and grid
fig = plt.figure(figsize=(10, 10))
gs = GridSpec(4, 2)
color = plt.cm.Dark2(0)  # Single color for the RFC plot

# Calibration curve plot
ax_calibration_curve = fig.add_subplot(gs[:2, :2])
CalibrationDisplay.from_predictions(
    y_test_offshore,
    y_prob,
    n_bins=10,
    name="Random Forest Classifier",
    ax=ax_calibration_curve,
    color=color,
)
ax_calibration_curve.grid()
ax_calibration_curve.set_title("Calibration Plot (Gradient Booster)")

# Histogram plot
ax_hist = fig.add_subplot(gs[2:, :])
ax_hist.hist(
    y_prob,
    range=(0, 1),
    bins=10,
    color=color,
    edgecolor='black'
)
ax_hist.set(
    title="Predicted Probability Distribution (GB)",
    xlabel="Mean Predicted Probability",
    ylabel="Count"
)

plt.tight_layout()
plt.show()

from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import brier_score_loss
#from sklearn.naive_bayes import GaussianNB

# With no calibration
prob_no_calib = gradient_booster.predict_proba(X_test_limited)[:, 1]

# With isotonic calibration
GB_isotonic = CalibratedClassifierCV(gradient_booster, cv=5, method="isotonic")
GB_isotonic.fit(X_train_limited, y_train_offshore)
prob_pos_isotonic = GB_isotonic.predict_proba(X_test_limited)[:, 1]

# With sigmoid calibration
GB_sigmoid = CalibratedClassifierCV(gradient_booster, cv=5, method="sigmoid")
GB_sigmoid.fit(X_train_limited, y_train_offshore)
prob_pos_sigmoid = GB_sigmoid.predict_proba(X_test_limited)[:, 1]

print("Brier score losses: (the smaller the better)")

clf_score = brier_score_loss(y_test_offshore, prob_no_calib)
print("No calibration: %1.3f" % clf_score)

GB_isotonic_score = brier_score_loss(y_test_offshore, prob_pos_isotonic)
print("With isotonic calibration: %1.3f" % GB_isotonic_score)

GB_sigmoid_score = brier_score_loss(y_test_offshore, prob_pos_sigmoid)
print("With sigmoid calibration: %1.3f" % GB_sigmoid_score)

"""# Support Vector Machine"""

from sklearn import svm




# create an instance of SVC
svc = svm.SVC( C=1500, # regularization
               kernel='rbf', # radial or gaussian kernel
               random_state=101,
               probability=True) # predict probabilities

svc.fit(X_train_limited, y_train_offshore) # fit data to SVC
y_pred_offshore = svc.predict(X_test_limited) # predict using same data



#Print Test Set Accuracy
print("\n---Test Set Accuracy")
print(svc.score(X_test_limited, y_test_offshore))

#Print Brier Score
print("\n---Brier Score")
# predict probabilities
probs = svc.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score
brier_raw = brier_score_loss(y_test_offshore, probs)
print(brier_raw)

#Print Brier Skill Score
print("\n---Brier Skill Score")
# predict probabilities
probs = svc.predict_proba(X_test_limited)
# keep the predictions for class 1 only
probs = probs[:, 1]
# calculate bier score

offshore_severe_pct_array = np.full(len(y_test_offshore), offshore_severe_pct)
brier_climo = brier_score_loss(y_test_offshore, offshore_severe_pct_array)
BSS = 1 - (brier_raw / brier_climo)
print(BSS)

#Confusion Matrix
print("\n---Confusion Matrix")
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test_offshore, y_pred_offshore)
print(confusion_matrix)

#Classification Report
print("\n---Classification Report")
from sklearn.metrics import classification_report
print(classification_report(y_test_offshore, y_pred_offshore))

# Make the confusion matrix

#cf_matrix = confusion_matrix(y_true, y_pred)
print("\nTest confusion_matrix")
sns.heatmap(confusion_matrix, annot=True, cmap='Blues')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)



"""# Conclusion

After testing several variations of logistic regression, decision trees, Random Forests, Gradient Boosters, and Support Vector Machines, the various models, I have decided to select a Random Forest model. For the version I selected I used a limited subset of the features based on several feature importance metrics. I chose class weights to be balanced, and then I further calibrated the probailistic output with isontonic calibration. This gave the lowest (best) brier score, and most reliable probabilistic outputs. I plan on implementing this model for the upcoming summer thunderstorm season to help forecast high impact weather for space launch at Cape Canaveral!
"""